{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d625b904-3f6a-473e-a038-f05d5c24f4c6",
   "metadata": {},
   "source": [
    "The goal of this notebook is to qualitatively replicates Figure 4a [1], where Ye &  Pandarinath (2021) showed that (Neural Data Transformers) NDT can learn a spike-to-rate-mapping in a synthetic dataset generated from the Lorenz equation. \n",
    "\n",
    "Summary: \n",
    "- A single-layer transformer can capture the spike-to-rate mapping in the Lorenz dataset fairly well but its performance is worse than the 6-layer transformer reported in Figure 4a [1]. \n",
    "\n",
    "\n",
    "Implementation detail: \n",
    "- One important difference is that the orignal paper [1] used a 6-layer transformer whereas I used a 1-layer transformer. Based on prior empirical results from [1], some performance decline is expected. \n",
    "- Since the NDT is a standard transformer architecture, I picked Keras since 1) the code can be more compact when implementing standard architectures and 2) this notebook can be executed with minimal dependencies. \n",
    "- Due to time constraints, I didn't not perform hyperparameter search like [1]. Instead, I mostly adpoted the hyperparameters from [1], such as the dropout rate, learning rate, number of attention heads, etc. On the other hand, for simplicity, code readability and speed (in case you wan to to run this notebook, it takes less than 2 mins), I did not adopt some of the training parameters, such as the number of training epochs, batch size, the way I normalize the data. I also simplified transformer masking data binning, etc. Despite these differences, the results does seem to replicate qualitatively. Overall, this replication is not meant to be an exact copy of NDT [1]. \n",
    "\n",
    "My implementation is mainly based on ... \n",
    "- [1] Ye, J., & Pandarinath, C. (2021). Representation learning for neural population activity with Neural Data Transformers. In arXiv [q-bio.NC]. arXiv. http://arxiv.org/abs/2108.01210\n",
    "    - Code: https://github.com/snel-repo/neural-data-transformers  \n",
    "    - For example, I modified/used the the h5 data loading function to load the Lorenz data (see utils.py).\n",
    "- [2] A keras tutorial of an End-to-end Masked Language Modeling with BERT: https://keras.io/examples/nlp/masked_language_modeling/\n",
    "\n",
    "by Qihong Lu (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14376a30-e676-416f-98a8-dae2d01f3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_data_from_h5, DATASET_MODES, Config\n",
    "from pprint import pprint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce274b-fa3f-4be1-8804-58e2bfa83fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load data'''\n",
    "fpath = \"data/lfads_lorenz.h5\"\n",
    "spikes_train, rates_train, heldout_spikes_train, forward_spikes_train = get_data_from_h5(\n",
    "    DATASET_MODES.train, fpath)\n",
    "spikes_val, rates_val, heldout_spikes_val, forward_spikes_val = get_data_from_h5(\n",
    "    DATASET_MODES.val, fpath)\n",
    "\n",
    "# normalize the data \n",
    "rates_train_n = (rates_train - rates_train.min()) / (rates_train.max() - rates_train.min())\n",
    "rates_val_n = (rates_val - rates_val.min()) / (rates_val.max() - rates_val.min())\n",
    "\n",
    "num_trials_train, trial_length, num_neurons = np.shape(spikes_train)\n",
    "num_trials_val, trial_length, num_neurons = np.shape(spikes_val)\n",
    "\n",
    "# simplification, to be fixed later\n",
    "config.MAX_LEN, config.EMBED_DIM = trial_length, num_neurons\n",
    "\n",
    "print(num_trials_train, trial_length, num_neurons)\n",
    "print(num_trials_val, trial_length, num_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7f70e-f8e6-445e-88f7-0d130bbb382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''build a transformer'''\n",
    "\n",
    "\n",
    "def bert_module(query, key, value):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_multiheadattention\",\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(config.DROPOUT_RATE, name=\"encoder_att_dropout\")(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=config.EPSILON, name=\"encoder_att_layernormalization\"\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_ffn\",\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(config.DROPOUT_RATE, name=\"encoder_ffn_dropout\")(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=config.EPSILON, name=\"encoder_ffn_layernormalization\"\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        features, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=None)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=None)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    # use the input spikes, remove the embedding, which is optional [1]\n",
    "    inputs = layers.Input((config.MAX_LEN, config.EMBED_DIM), dtype=tf.float32)\n",
    "    i2h = layers.Dense(config.EMBED_DIM, activation='relu', input_shape=(config.MAX_LEN,))(inputs)\n",
    "    # positional embedding\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = inputs + position_embeddings    \n",
    "    encoder_output = bert_module(embeddings, embeddings, embeddings)\n",
    "    mlm_output = layers.Dense(config.EMBED_DIM, name=\"mlm_reg\", activation=\"sigmoid\")(encoder_output)\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72edb4f7-e31d-4328-b41e-d80cd731d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f133a1-dcbd-414e-a805-74fef0685420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train and Save \"\"\"\n",
    "n_epochs = 50\n",
    "\n",
    "x_i, y_i = spikes_train, rates_train_n\n",
    "x_i, y_i = tf.convert_to_tensor(x_i), tf.convert_to_tensor(y_i)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    bert_masked_model.fit(x_i, y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e306a8d-5a0f-4a30-b376-d4283b8a5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plotting params'''\n",
    "cp = sns.color_palette()\n",
    "sns.set(style='white', palette='colorblind', context='poster')\n",
    "alpha = .5\n",
    "\n",
    "fig_path = 'figs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da1839-8df6-441d-a245-3003ab4cb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on the validation set\n",
    "\n",
    "x_val, y_val = tf.convert_to_tensor(spikes_val), rates_val_n\n",
    "yhat = bert_masked_model.predict(x_val)\n",
    "\n",
    "# pick the ith trial\n",
    "i = 0 \n",
    "\n",
    "vmax = np.max([np.max(yhat), np.max(y_val)])\n",
    "vmin = np.min([np.min(yhat), np.min(y_val)])\n",
    "f, axes = plt.subplots(1,2,  figsize=(15,6))\n",
    "sns.heatmap(yhat[i].T,ax=axes[0], cmap='viridis',vmax=vmax,vmin=vmin)\n",
    "sns.heatmap(y_val[i].T,ax=axes[1], cmap='viridis',vmax=vmax,vmin=vmin)\n",
    "axes[0].set_title('Normalized rates, predicted')\n",
    "axes[0].set_ylabel('Trial length')\n",
    "axes[0].set_xlabel('Neurons')\n",
    "axes[1].set_title('Normalized rates, ground truth')\n",
    "axes[1].set_ylabel('Trial length')\n",
    "axes[1].set_xlabel('Neurons')\n",
    "f.tight_layout()\n",
    "\n",
    "fig_name = 'rates_heatmap'\n",
    "f.savefig(fig_path + fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712362e-623c-4aff-87d7-481d8296eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the jth neuron\n",
    "j = 0 \n",
    "n_trial_to_plot = 5\n",
    "\n",
    "f, ax = plt.subplots(1,1,  figsize=(12,6))\n",
    "for i in range(n_trial_to_plot):\n",
    "    if i == 0: \n",
    "        label_yhat, label_y = 'Predicted', 'Ground truth'\n",
    "    else:\n",
    "        label_yhat, label_y = None, None        \n",
    "    \n",
    "    ax.plot(yhat[i][:,j], label = label_yhat, color = cp[0], alpha = alpha)\n",
    "    ax.plot(y_val[i][:,j], label = label_y, color = 'k', alpha = alpha)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Rates, normalized')\n",
    "    ax.set_title(f'{n_trial_to_plot} example trials from the Lorenz data')    \n",
    "    ax.legend()\n",
    "    sns.despine()\n",
    "\n",
    "fig_name = 'rates_fig4a1'\n",
    "f.savefig(fig_path + fig_name)    \n",
    "    \n",
    "f, ax = plt.subplots(1,1,  figsize=(15,3))\n",
    "sns.heatmap(spikes_val[i][:,:n_trial_to_plot].T, cmap='bone_r')\n",
    "ax.set_ylabel('Trials \\n\\n')\n",
    "ax.get_xaxis().set_ticks([])\n",
    "ax.get_yaxis().set_ticks([])\n",
    "\n",
    "fig_name = 'rates_fig4a2'\n",
    "f.savefig(fig_path + fig_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdec37c-9978-4243-99f9-86549e606b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
